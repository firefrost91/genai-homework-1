# 94-844 Generative AI Lab (Fall 2025) â€” Assignment #1  
### Prompt Engineering with LLMs  
Prof. Sara Kingsley Â· Heinz College Â· Carnegie Mellon University  

**Team Members:** Ansh , Arthur, Jiayi 
**Due Date:** November 2, 2025 Â· **Weight:** 20% (Part A 10% + Part B 10%)

---

## Overview

Prompt engineering is the process of designing and refining inputs (prompts) to elicit desired outputs from a large language model (LLM).  
This assignment explores how prompt structure, specificity, and reasoning style affect LLM performance and reliability.

Our work is divided into two parts:

| Section | Focus | Description |
|----------|--------|-------------|
| **Part A â€” Prompt Design Experiments** | Design & Analysis | Created and tested five distinct prompt structures across multiple tasks to observe how prompt framing changes model outputs. |
| **Part B â€” Zero, Few-Shot & Chain-of-Thought Prompting** | Temperature Sweep & Performance Study | Evaluated LLM accuracy under different prompt styles and sampling temperatures using the HellaSWAG reasoning benchmark. |

---

## ðŸ§© Part A â€” Experimenting with Prompt Design (10%)

### ðŸŽ¯ Objectives
1. Design five diverse prompts for each task that differ in structure and style.  
2. Submit each prompt to the OpenAI API and record model outputs.  
3. Analyze which prompt styles perform best and why.  
4. Derive best practices for prompt crafting based on observed LLM behavior.

### ðŸ§  Tasks Explored
- **Summarization:** condensing reports into concise insights - analysis done by Jiayi
- **Creative Writing:** open-ended story construction - analysis done by Arthur
- **Question Answering:** fact-based comprehension  - analysis done by Ansh


Each taskâ€™s results are analyzed for:
- Accuracy / quality of generation  
- Hallucination rates  
- Sensitivity to instruction phrasing  

### ðŸ§¾ Deliverables
- **`part_a_experiments.ipynb`** â€” Colab notebook with API calls, outputs, and visual examples.  
- **`part_a_report.pdf`** â€” Includes analysis tables, discussion of patterns, and derived prompt engineering principles.

---

## ðŸ”¬ Part B â€” Zero, Few-Shot & Chain-of-Thought Prompting (10%)

### ðŸŽ¯ Objective
To measure how different prompting strategies and temperature settings affect LLM reasoning accuracy on a multiple-choice benchmark task.

We experimented with:
1. **Baseline Prompt** â€” Direct question with no examples.  
2. **1-Shot Prompt** â€” Includes one example demonstration.  
3. **3-Shot Prompt** â€” Includes three example demonstrations.  
4. **5-Shot Prompt** â€” Five example demonstrations (standard few-shot).  
5. **Chain-of-Thought (CoT)** â€” Explicit reasoning before final answer.  
6. **Custom Variants** â€” Example order shuffled and mislabeled versions.

### ðŸ§® Dataset
- **Benchmark:** HellaSWAG (validation split)  
- **Subset:** Randomly sampled instances for speed and reproducibility  
- **Goal:** Evaluate each prompt type on the same set of questions  

### ðŸ§° Code Overview

#### `run_experiments()`
Runs all prompt types for a given temperature and iteration; records accuracy and model outputs.

#### `run_temperature_variance()`
Sweeps across temperatures ( `[0.0, 0.4, 0.6, 1.0]` ) and aggregates results for visual analysis.



Results are saved and auto-downloaded as:

